{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3e7d830-778b-4640-98e7-de5be61ca7af",
   "metadata": {},
   "source": [
    "## Ames Wang\n",
    "## Homework 4\n",
    "## 11/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36121527-a7bb-4804-a7ac-b7cbe9c1d3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By # used to import different ways to access data in the XML or HTML file\n",
    "from selenium.webdriver.chrome.service import Service # no longer need to download a driver file, use service\n",
    "from webdriver_manager.chrome import ChromeDriverManager # used to manage the Chrome driver to emulate a Chrome web browser\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10441305-de05-4541-8b6d-284b1dd301c3",
   "metadata": {},
   "source": [
    "## Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8378492b-e5ca-45ee-804e-92a799df3406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page: https://www.scrapethissite.com/pages/forms/?page_num=1\n",
      "Page 1 done\n",
      "Scraping page: https://www.scrapethissite.com/pages/forms/?page_num=2\n",
      "Page 2 done\n",
      "Scraping page: https://www.scrapethissite.com/pages/forms/?page_num=3\n",
      "Page 3 done\n",
      "Scraping page: https://www.scrapethissite.com/pages/forms/?page_num=4\n",
      "Page 4 done\n",
      "Scraping page: https://www.scrapethissite.com/pages/forms/?page_num=5\n",
      "Page 5 done\n",
      "Scraping page: https://www.scrapethissite.com/pages/forms/?page_num=6\n",
      "Page 6 done\n",
      "Scraping page: https://www.scrapethissite.com/pages/forms/?page_num=7\n",
      "Page 7 done\n",
      "Scraping page: https://www.scrapethissite.com/pages/forms/?page_num=8\n",
      "Page 8 done\n",
      "Scraping page: https://www.scrapethissite.com/pages/forms/?page_num=9\n",
      "Page 9 done\n",
      "Scraping page: https://www.scrapethissite.com/pages/forms/?page_num=10\n",
      "Page 10 done\n",
      "Scraping page: https://www.scrapethissite.com/pages/forms/?page_num=11\n",
      "Page 11 done\n",
      "Scraping page: https://www.scrapethissite.com/pages/forms/?page_num=12\n",
      "Page 12 done\n",
      "Scraping page: https://www.scrapethissite.com/pages/forms/?page_num=13\n",
      "Page 13 done\n",
      "Scraping page: https://www.scrapethissite.com/pages/forms/?page_num=14\n",
      "Page 14 done\n",
      "Scraping page: https://www.scrapethissite.com/pages/forms/?page_num=15\n"
     ]
    }
   ],
   "source": [
    "#Function to perform random scrolling\n",
    "def random_scroll(browser, total_wait_time):\n",
    "    total_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "    scroll_steps = random.randint(3, 10)\n",
    "    scroll_increment = total_height // scroll_steps\n",
    "    time_per_step = total_wait_time / scroll_steps\n",
    "    \n",
    "    for step in range(scroll_steps):\n",
    "        browser.execute_script(f\"window.scrollBy(0, {scroll_increment});\")\n",
    "        random_wait = random.uniform(0.5 * time_per_step, 1.5 * time_per_step)\n",
    "        time.sleep(random_wait)\n",
    "    \n",
    "    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "# Initialize the WebDriver (make sure to have the appropriate driver installed)\n",
    "browser = webdriver.Chrome()\n",
    "teams = []\n",
    "years = []\n",
    "wins = []\n",
    "losses = []\n",
    "\n",
    "# Loop through the NHL pages (update range to the actual number of pages)\n",
    "for i in range(1, 25):  # Adjust the range based on the actual number of pages\n",
    "    url = f\"https://www.scrapethissite.com/pages/forms/?page_num={i}\"\n",
    "    print(f\"Scraping page: {url}\")\n",
    "\n",
    "    browser.get(url)\n",
    "\n",
    "    total_wait_time = random.uniform(2, 20)\n",
    "    random_scroll(browser, total_wait_time)\n",
    "\n",
    "    # Extract the NHL team data\n",
    "    team_elements = browser.find_elements(By.CLASS_NAME, \"team\")\n",
    "    \n",
    "    for team in team_elements:\n",
    "        try:\n",
    "            # Get the team name\n",
    "            team_name = team.find_element(By.CLASS_NAME, \"name\").text\n",
    "            teams.append(team_name)\n",
    "            \n",
    "            # Get the year\n",
    "            year = team.find_element(By.CLASS_NAME, \"year\").text\n",
    "            years.append(year)\n",
    "            \n",
    "            # Get the number of wins\n",
    "            win = team.find_element(By.CLASS_NAME, \"wins\").text\n",
    "            wins.append(win)\n",
    "            \n",
    "            # Get the number of losses\n",
    "            loss = team.find_element(By.CLASS_NAME, \"losses\").text\n",
    "            losses.append(loss)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting data for team: {e}\")\n",
    "\n",
    "    print(f\"Page {i} done\")\n",
    "\n",
    "    # Add delay before moving to the next page\n",
    "    time.sleep(random.uniform(2, 5))\n",
    "\n",
    "# Close the browser\n",
    "browser.close()\n",
    "\n",
    "# Convert the data into a DataFrame\n",
    "nhl_data = pd.DataFrame({\n",
    "    \"Team\": teams,\n",
    "    \"Year\": years,\n",
    "    \"Wins\": wins,\n",
    "    \"Losses\": losses\n",
    "})\n",
    "\n",
    "# Save the data as a CSV file\n",
    "nhl_data.to_csv(\"nhl_scraped_raw.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"DataFrame shape: {nhl_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e07f8f-c2e6-41ca-90ba-ef4cd820356b",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cc8453-1701-4888-bf20-333be1959493",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Clean and transform nhl_data\n",
    "# Clean up the data (remove \\n and leading/trailing whitespace)\n",
    "nhl_data['Team'] = nhl_data['Team'].str.replace('\\n', '').str.strip()\n",
    "nhl_data['Year'] = nhl_data['Year'].astype(str).str.replace('\\n', '').str.strip()\n",
    "nhl_data['Wins'] = nhl_data['Wins'].astype(str).str.replace('\\n', '').str.strip()\n",
    "nhl_data['Losses'] = nhl_data['Losses'].astype(str).str.replace('\\n', '').str.strip()\n",
    "\n",
    "# Convert Year, Wins, and Losses into integer data types (int64)\n",
    "nhl_data['Year'] = nhl_data['Year'].astype('int64')\n",
    "nhl_data['Wins'] = nhl_data['Wins'].astype('int64')\n",
    "nhl_data['Losses'] = nhl_data['Losses'].astype('int64')\n",
    "\n",
    "# Print the data types to verify\n",
    "print(nhl_data.dtypes)\n",
    "\n",
    "# Save the DataFrame to a CSV file with UTF-8 encoding\n",
    "nhl_data.to_csv('nhl_scraped_raw.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"DataFrame shape: {nhl_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700cc26f-e62e-4260-b7ed-2ec5b1abe508",
   "metadata": {},
   "source": [
    "## Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23017777-b7b8-4293-ba1d-30447ae93b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "nhl_expanded = pd.read_excel(\"nhl_2012-2021.xlsx\")\n",
    "\n",
    "# Check and rename columns if necessary\n",
    "nhl_expanded.rename(columns={\n",
    "    'Season': 'Year',  # Rename 'Season' to 'Year'\n",
    "    'W': 'Wins',  # Replace 'ColumnE' with the actual column name for Wins\n",
    "    'L': 'Losses',  # Replace 'ColumnF' with the actual column name for Losses\n",
    "}, inplace=True)\n",
    "\n",
    "# Clean up the data (remove trailing * from team names)\n",
    "nhl_expanded['Team'] = nhl_expanded['Team'].str.rstrip('*').str.strip()\n",
    "\n",
    "# Vertically merge DataFrames\n",
    "nhl2 = pd.concat([nhl_data, nhl_expanded], ignore_index=True)\n",
    "\n",
    "# Create a new calculated column for win percentage\n",
    "nhl2['win%'] = nhl2['Wins'] / (nhl2['Wins'] + nhl2['Losses'])\n",
    "\n",
    "# SKeep only the necessary columns\n",
    "nhl2 = nhl2[['Team', 'Year', 'Wins', 'Losses', 'win%']]\n",
    "\n",
    "# Print the shape of the final DataFrame\n",
    "print(f\"Shape of the final DataFrame: {nhl2.shape}\")\n",
    "\n",
    "# Print the first few rows of the final DataFrame to verify the contents\n",
    "print(nhl2.head())\n",
    "\n",
    "# Save the final DataFrame to a CSV file with UTF-8 encoding\n",
    "nhl2.to_csv('nhl_final.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f8a253-e517-481e-9909-63ebd84ed0ba",
   "metadata": {},
   "source": [
    "## Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2944106-ddff-453b-bc2e-d263b0c28908",
   "metadata": {},
   "outputs": [],
   "source": [
    "arena = pd.read_csv(\"nhl_hockey_arenas.csv\")\n",
    "arena.rename(columns={\n",
    "    'Team Name': 'Team',  # Rename 'Team Name' to 'Team'\n",
    "    'Arena Name': 'Arena',\n",
    "    'Arena Location': 'Location',\n",
    "    'Seating Capacity': 'Capacity'\n",
    "}, inplace=True)\n",
    "\n",
    "print(\"Unique team names in nhl2:\", nhl2['Team'].unique())\n",
    "print(\"Unique team names in arena:\", arena['Team'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4932d0b0-76a4-4b7b-8847-145899402c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct any discrepancies\n",
    "nhl2['Team'] = nhl2['Team'].replace('Mighty Ducks of Anaheim', 'Anaheim Ducks')\n",
    "nhl2['Team'] = nhl2['Team'].replace('Seattle Kraken', 'Seattle Kracken')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf45a89d-c2e6-471f-9198-3ec2c9eaebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform an inner join between nhl2 and arena\n",
    "nhl3 = pd.merge(nhl2, arena[['Team', 'Arena', 'Location', 'Capacity']], on='Team', how='inner')\n",
    "\n",
    "# Print the shape of the final DataFrame\n",
    "print(f\"Shape of the final DataFrame: {nhl3.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290c0bcd-df6d-4b02-b358-cda174388b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final DataFrame to a CSV file with UTF-8 encoding\n",
    "nhl3.to_csv('nhl_final.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b2c2b9-14a8-4a3b-bb13-f6a80fbb5b38",
   "metadata": {},
   "source": [
    "## Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabc7c6e-c339-4607-9c02-4d3c48f46c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final DataFrame to a CSV file with UTF-8 encoding\n",
    "nhl3.to_csv('awang24_hockey_data.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45439be-1c6a-48d8-9863-d90c09a0335b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f63248d-50c7-47ed-8f1d-b5e89e5f7ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639e5315-570f-4f36-967a-75c800e521dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
